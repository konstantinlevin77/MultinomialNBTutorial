{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767649b5",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes From Scratch\n",
    "\n",
    "<div id=\"1\"> </div>\n",
    "    \n",
    "## 1. Introduction\n",
    "\n",
    "Hello everyone, welcome to another 'from scratch' notebook. Today, we'll implement Multinomial Naive Bayes from scratch. Don't worry if you're not familiar with the algorithm, I'll try to break it into smaller parts. Let's get started.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [1. Introduction](#1)\n",
    "* [2. Meet With The Data](#2)\n",
    "* [3. Naive Bayes](#3)\n",
    "    * [3.1 What is Naive Bayes](#3.)\n",
    "    * [3.2 Calculating Posterior Probabilities](#3.2)\n",
    "    * [3.3 Questions we should think about](#3.3)\n",
    "    * [3.4 Summary With The Terminology](#3.4)\n",
    "* [4. Implementation](#4)\n",
    "    * [4.1 Calculating Prior Probabilities](#4.1)\n",
    "    * [4.2 Calculating Conditional Probabilities](#4.2)\n",
    "    * [4.3 Calculating Posterior Probabilities For New Samples](#4.3)\n",
    "    * [4.4 Prediction Function and Testing](#4.4)\n",
    "* [5. Conclusion](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf308b9",
   "metadata": {},
   "source": [
    "<div id=\"2\">\n",
    "\n",
    "## 2. Meet With The Data\n",
    "\n",
    "We'll start by getting to know the dataset we have. That will help us grasp concepts later, I guess :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1103594a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>0800</th>\n",
       "      <th>08000839402</th>\n",
       "      <th>08000930705</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10p</th>\n",
       "      <th>...</th>\n",
       "      <th>yo</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>ì_</th>\n",
       "      <th>ìï</th>\n",
       "      <th>û_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  03  04  0800  08000839402  08000930705  10  100  1000  10p  ...  yo  \\\n",
       "0    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "1    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "2    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "3    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "4    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "\n",
       "   you  your  yours  yourself  yr  yup  ì_  ìï  û_  \n",
       "0    0     0      0         0   0    0   0   0   0  \n",
       "1    0     0      0         0   0    0   0   0   0  \n",
       "2    0     0      0         0   0    0   0   0   0  \n",
       "3    0     0      0         0   0    0   0   0   0  \n",
       "4    0     0      0         0   0    0   0   0   0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = pd.read_csv(\"processed_sms_spam.csv\",encoding=\"latin1\")\n",
    "X.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "y = np.load(\"y.npy\")\n",
    "\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77becfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Columns: 1000 entries, 000 to û_\n",
      "dtypes: int64(1000)\n",
      "memory usage: 42.5 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd40cd4",
   "metadata": {},
   "source": [
    "* The dataset has 1000 features and a total of 5572 entries.\n",
    "\n",
    "Even though it doesn't look like that, this is a text dataset. But as you guess, it's preprocessed before, to make tutorial simpler :)\n",
    "\n",
    "This is the famous [SMS Spam Dataset of UCI Machine Learning](https://archive.ics.uci.edu/dataset/228/sms+spam+collection). Original entries look like this:\n",
    "\n",
    "1. `ham`\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "2. `ham`\tOk lar... Joking wif u oni...\n",
    "3. `spam`\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "\n",
    "**In order to make it structured, I've made a table that shows how many times each *most used 1000* words are seen in each sample. Let's look at it again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff611f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>0800</th>\n",
       "      <th>08000839402</th>\n",
       "      <th>08000930705</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10p</th>\n",
       "      <th>...</th>\n",
       "      <th>yo</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>ì_</th>\n",
       "      <th>ìï</th>\n",
       "      <th>û_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  03  04  0800  08000839402  08000930705  10  100  1000  10p  ...  yo  \\\n",
       "0    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "1    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "2    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "3    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "4    0   0   0     0            0            0   0    0     0    0  ...   0   \n",
       "\n",
       "   you  your  yours  yourself  yr  yup  ì_  ìï  û_  \n",
       "0    0     0      0         0   0    0   0   0   0  \n",
       "1    0     0      0         0   0    0   0   0   0  \n",
       "2    0     0      0         0   0    0   0   0   0  \n",
       "3    0     0      0         0   0    0   0   0   0  \n",
       "4    0     0      0         0   0    0   0   0   0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778dd1d",
   "metadata": {},
   "source": [
    "* Such as the first entry doesn't involve words \"you\",\"your\",\"yours\" etc. Let's see the entire row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd5e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6abec",
   "metadata": {},
   "source": [
    "* It's possible to see some ones all around the array. Those are the words that the original entry has.\n",
    "* If you want to know more about how to process text datasets like this, this is called as TF (Term Frequency) and sklearn has an implementation, you can look at it [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dceb1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d49c58",
   "metadata": {},
   "source": [
    "<div id=\"3\">\n",
    "\n",
    "## 3. Naive Bayes\n",
    "\n",
    "\n",
    "### 3.1 What is Naive Bayes?\n",
    "Alright, we met with the data and now we can talk about the algorithm.  Naive Bayes is a probabilistic classification algorithm generally used with text data. It's all about probabilities and today we'll talk about them.\n",
    "\n",
    "\n",
    "* Let's imagine we have an SMS as follows: `Give money, now!`, and we want to classify it as spam or not spam. Naive Bayes calculates the possibility of the message being `spam`. Then it calculates the possibility of message being `not spam`. We choose the one which has a higher possibility.\n",
    "\n",
    "Here is the mathematical representation of what I said:\n",
    "\n",
    "We first calculate $P(\\text{give money now} | \\text{spam})$ \n",
    "\n",
    "* If you're not familiar with the probability notation, this means that: **given** that the message is spam, probability of \"give money now\". We can interpret that as the possibility of \"give money now\" being spam.\n",
    "\n",
    "Then we calculate $P(\\text{give money now} | \\text{ham})$\n",
    "\n",
    "* And as we said earlier, this is read as **given** that the message is ham, probability of \"give money now\"\n",
    "\n",
    "Now let'say $P(\\text{give money now} | \\text{spam}) = 0.8$ and $P(\\text{give money now} | \\text{ham}) = 0.2$. Since spam has a higher probability, we can say that the message is more likely to be a spam.\n",
    "\n",
    "\n",
    "**TERMINOLOGY:** Probability of the message being spam or not spam  $P(\\text{give money now} | \\text{spam})$  or $P(\\text{give money now} | \\text{ham})$ is called as **posterior probability**. And our goal is to calculate posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e574c",
   "metadata": {},
   "source": [
    "<div id=\"3.2\">\n",
    "\n",
    "### 3.2 Calculating Posterior Probabilities\n",
    "\n",
    "Now let's see how to calculate posterior probabilities. In order to calculate a posterior probability, we need a few other probabilities. Let's see it with the same example.\n",
    "\n",
    "Posterior probability of the SMS being spam is calculated as follows:\n",
    "\n",
    "$$ P(\\text{give money now} | \\text{spam}) = P(\\text{spam}) \\cdot P(\\text{give} | \\text{spam}) \\cdot P(\\text{money} | \\text{spam}) \\cdot P(\\text{now} | \\text{spam})$$\n",
    "\n",
    "1. $P(\\text{spam})$ is the probability of any message in the dataset being spam. Such as if we had a dataset with 10 ten positive and 40 negative samples, $P(\\text{spam})$ would be 0.2 and as you guess, $P(\\text{ham})$ would be 0.8.\n",
    "\n",
    "<div></div>\n",
    "\n",
    "2. $P(\\text{give} | \\text{spam})$,$P(\\text{money} | \\text{spam})$,$P(\\text{now} | \\text{spam})$ are probabilities of each word appearing in the spam messages. We can calculate them as follows:\n",
    "\n",
    "$$P(\\text{any word} | \\text{any class}) = {\\text{Number of times the word appears} \\over \\text{Total number of words in the class}} $$\n",
    "\n",
    "\n",
    "For example, if there are a total of 100 words in the all spam entrie and the word `give` occurs 10 times in the spam entries, $P(\\text{give} | \\text{spam})$ would be 0.1 Same applies to other words and not spam entries as well. \n",
    "\n",
    "<div></div> \n",
    "\n",
    "Now let's just get familiar with the terminology\n",
    "\n",
    "**TERMINOLOGY:** \n",
    "1. $P(\\text{spam})$ and $P(\\text{ham})$ are called as **prior probability**\n",
    "2. $P(\\text{give} | \\text{spam})$,  $P(\\text{money} | \\text{spam})$,  $P(\\text{now} | \\text{spam})$ are called as **conditional probabilities** or **feature probabilities**\n",
    "\n",
    "That's all we need to know about the basics of Naive Bayes, now we'll learn the answers of some questions and then start to implement the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1329ee",
   "metadata": {},
   "source": [
    "<div id=\"3.3\">\n",
    "\n",
    "### 3.3 Questions we should think about\n",
    "\n",
    "\n",
    "#### 3.3.1 What if a word doesn't appear in spam or not spam?\n",
    "\n",
    "Let's think another example: `Appalachia money money money give us` This message seems to be spam, but let's use our algorithm to know if it is.\n",
    "\n",
    "We want to calculate posterior probability: $$P(\\text{Appalachia money money money give us} | \\text{spam})$$\n",
    "It's calculated as:\n",
    "\n",
    "$$P(\\text{Appalachia money money money give us} | \\text{spam}) = P(\\text{spam}) \\cdot P(\\text{appalachia} | \\text{spam}) \\cdot P(\\text{money} | \\text{spam})^3 \\cdot P(\\text{give} | \\text{spam}) \\cdot P(\\text{us} | \\text{spam})$$ \n",
    "\n",
    "Before we start with the prior probability, a word catches our attention: Appalachia. When we check our dataset to see if the word Appalachia appears in any of the spam entries, we see that it never does. So $P(\\text{appalachia} | \\text{spam})$ is simply 0. Result is always 0 when we multiply 0 with other numbers so the posterior probability is 0. According to the algorithm, this message can't be a spam. However, this feels wrong. ***Most of the words raise suspect and just because we've never seen the word Appalachia in the spam messages, we can't actually say that it's not spam.***\n",
    "\n",
    "So in order to solve this problem, we apply a method called **Laplace smoothing** or **Additive smoothing**. It's actually simple, when we calculate the feature probability of a word, we always add a small number (like 1) to **both nominator and denominator**, so the probability is never 0.\n",
    "\n",
    "For example, before we've calculated the conditional probability of appalachia as:\n",
    "\n",
    "$$P(\\text{appalachia} | \\text{spam}) = {\\text{count of appalachia in the spam entries} \\over \\text{count of words in the spam entries}} = {0 \\over \\text{some number}} = 0$$\n",
    "\n",
    "Now, when we calculate it with laplace smoothing:\n",
    "\n",
    "$$P(\\text{appalachia} | \\text{spam}) = {\\text{count of appalachia in the spam entries + 1} \\over \\text{count of words in the spam entries + 1 * number of unique words}} = {1 \\over \\text{some number + some other number}} = \\text{A really small number like } 0.002$$\n",
    "    \n",
    "* We added $1 * \\text{number of unique words}$ instead of adding 1 to denominator. Because we add 1 to nominator each time we calculate the posterior probability of a word and they sum up as $1 * \\text{number of unique words}$.\n",
    "So we add that to denominator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998061e",
   "metadata": {},
   "source": [
    "####  3.3.2 Are there any problems we may encounter when we multiply small numbers like probabilities?\n",
    "\n",
    "Unfortunately, there is. As it's stated in the question, probabilities are small values between 0 and 1 When you multiply probabilities with each other result gets smaller and smaller. And once it achieves a really small value like 0.0000000000000004, computers can't track it anymore. \n",
    "\n",
    "Luckily, we have a solution. We apply $\\log$ transformation to the multiplication. Like instead of calculating $0.1 \\cdot 0.1 \\cdot 0.3$, we calculate $\\log(0.1 \\cdot 0.1 \\cdot 0.3 )$\n",
    "\n",
    "You may ask that how that does help with multiplying small numbers, since it looks like we have to multiply them as well to apply logarithm. However, logarithms have an incredible feature:\n",
    "\n",
    "$\\log(0.1 \\cdot 0.1 \\cdot 0.3 )$ can be written as $\\log(0.1) + \\log(0.1) + \\log(0.3)$ Let's see this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9dccaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.809142990314027"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.1 * 0.1 * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69688187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.809142990314027"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.1) + np.log(0.1) + np.log(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e26f4",
   "metadata": {},
   "source": [
    "* You may also say that now they don't look like probabilities anymore and you might be right about it. However they still represent the same idea, they just changed to log space! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407103b",
   "metadata": {},
   "source": [
    "#### 3.3.3 What does the naive in Naive Bayes mean?\n",
    "\n",
    "We've talked about the Naive Bayes a lot, but didn't discuss what the name means. Since Naive Bayes heavily depens on the bayesian statistics, the term Bayes come from Bayesian Statistics. What matters more is definitely the word *Naive* What does naive actually mean in this context?\n",
    "\n",
    "\n",
    "The word naive demonstrates that **each feature follows the same type of distribution (like gaussian, bernoulli, multinomial etc.) and each feature is independent from each other** \n",
    "\n",
    "Unfortunately most of the real word data includes features that are more or less dependent to each other. Buuut, Naive Bayes surprisingly handles those datasets without too much problem. Thus, we can always keep Naive Bayes in mind as an option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e51ec10",
   "metadata": {},
   "source": [
    "<div id=\"3.4\">\n",
    "\n",
    "### 3.4 Summary With The Terminology\n",
    "\n",
    "Before we implement the algorithm, let's summarize it with the terminology. \n",
    "\n",
    "1. Naive Bayes calculates **posterior probabilities**, probabilities of the messages being spam or not spam.\n",
    "\n",
    "<div>\n",
    "\n",
    "2. We multiply **prior probability** and **conditional probabilities** to calculate posterior probability\n",
    "    1. Prior probability is the probability of a class (spam and not spam in this case) appearing in the dataset.\n",
    "    2. Conditional probability or feature probability is the probability of a word appearing in a class.\n",
    "\n",
    "<div>\n",
    "\n",
    "3. When a word doesn't appear in a class, Naive Bayes thinks that that message can't belong to that class since this situation hasn't seen before. In order to solve this, we apply a method called **Laplace smoothing** while we calculate conditional probabilities.\n",
    "\n",
    "<div>\n",
    "    \n",
    "4. As we said, we multiply prior probability and conditional probabilities to calculate posterior probability. However, probabilities are often small numbers and when you multiply too much small numbers, results sometimes get so small and computers lose the track of it. We solve this problem by **applying logarithms**. Logarithms have an incredible feature that helps us solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98675497",
   "metadata": {},
   "source": [
    "<div id=\"4\">\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "We're ready to implement Naive Bayes, let's get started. We'll start by calculating prior probabilities for each class.\n",
    "\n",
    "<div id=\"4.1\">\n",
    "    \n",
    "### 4.1 Calculating Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546bd1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior_probabilities(y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates prior probability of each class. P(class)\n",
    "    Args: y (ndarray) (m,) labels of m examples. \n",
    "    Labels should be integers,sequential and they should start from 0.\n",
    "    \n",
    "    Returns: prior_probabilities (ndarray) (n,) prior probability of each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    num_samples = y.shape[0]\n",
    "    \n",
    "    prior_probabilities = np.zeros((len(classes),))\n",
    "    \n",
    "    for class_ in classes:\n",
    "        \n",
    "        # np.where() returns the indices the condition given is met.\n",
    "        sample_count = np.where(y == class_)[0].shape[0]\n",
    "        \n",
    "        prior_probabilities[class_] = sample_count / num_samples\n",
    "    \n",
    "    return prior_probabilities\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3c5e9",
   "metadata": {},
   "source": [
    "* Let's see the prior probabilities of each class!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a822db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86593683, 0.13406317])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_prior_probabilities(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6eda51",
   "metadata": {},
   "source": [
    "* Since most of the dataset (roughly 86%) consists of not spam entries, prior probability of not spam is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c932fa",
   "metadata": {},
   "source": [
    "<div id=\"4.2\">\n",
    "\n",
    "### 4.2 Calculating Conditional Probabilities\n",
    "\n",
    "Now, we'll define a function to calculate conditional probabilities of each words for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e74b4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_probabilities(X,y,alpha=1.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates probability of each feature given each class, P(feature | class)\n",
    "    Args:\n",
    "    X (ndarray) (m,n) m examples with n features\n",
    "    y (ndarray) (m,) labels of m examples\n",
    "    alpha (scalar) laplace smoothing parameter, default: 1.0\n",
    "    \n",
    "    Returns:\n",
    "    feature_probabilities (ndarray) (n,num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    m,n = X.shape\n",
    "    num_classes = len(np.unique(y))\n",
    "    \n",
    "    feature_probabilities = np.zeros([n,num_classes])\n",
    "    \n",
    "    # Iterate through classes\n",
    "    for class_i in range(num_classes):\n",
    "        \n",
    "        \n",
    "        class_indices = np.where(y == class_i)[0]\n",
    "        X_class = X[class_indices]\n",
    "        \n",
    "        # Number of words in this class (denominator)\n",
    "        total_count = np.sum(X_class)\n",
    "        \n",
    "        # Laplace smoothing applied to the denominator\n",
    "        # Since we add _alpha_ to all words, we add _alpha_ times n to denominator\n",
    "        smoothed_total_count = total_count + alpha * n\n",
    "        \n",
    "        \n",
    "        for word_i in range(n):\n",
    "            \n",
    "            # Number of times this word appeared in this class. (nominator)\n",
    "            word_class_count = np.sum(X_class[:,word_i])\n",
    "            \n",
    "            # Laplace smoothing applied to nominator.\n",
    "            smoothed_word_class_count = word_class_count + alpha\n",
    "            \n",
    "            feature_probabilities[word_i,class_i] = smoothed_word_class_count / smoothed_total_count\n",
    "    \n",
    "    \n",
    "    return feature_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6edaab",
   "metadata": {},
   "source": [
    "* Now let's see the feature probabilities of a word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ad8e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Word 21 | Spam) is: 0.000924477314749\n",
      "P(Word 21 | Not Spam) is: 0.000019359959731\n"
     ]
    }
   ],
   "source": [
    "f_probs = calculate_feature_probabilities(X,y,alpha=1.0)\n",
    "\n",
    "word_21_spam = f_probs[20][1]\n",
    "word_21_not_spam = f_probs[20][0]\n",
    "\n",
    "print(\"P(Word 21 | Spam) is: {:.15f}\".format(word_21_spam))\n",
    "print(\"P(Word 21 | Not Spam) is: {:.15f}\".format(word_21_not_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ed06a",
   "metadata": {},
   "source": [
    "* Probability of Word 21 appearing in the spam messages is higher than not spam messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99723d9c",
   "metadata": {},
   "source": [
    "<div id=\"4.3\">\n",
    "\n",
    "### 4.3 Calculating Posterior Probabilities For New Samples\n",
    "\n",
    "Now we calculated prior and conditional probabilities, we can calculate posterior probabilities for new samples. Now we'll implement a function that calculates the posterior probability for one sample. It'll calculate it for both classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb756d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior_probabilities(x,prior_probabilities,feature_probabilities):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates posterior probabilities of one sample of each class P(sample | class )\n",
    "    Args:\n",
    "    x ndarray (n,) one sample\n",
    "    prior_probabilities (ndarray) (num_classes,)\n",
    "    feature_probabilities (ndarray) (n,num_classes)\n",
    "    \n",
    "    Returns:\n",
    "    log_posterior_probabilities (ndarray) (num_classes,)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_classes = len(prior_probabilities)\n",
    "    \n",
    "    log_posterior_probabilities = np.zeros((num_classes,)) \n",
    "    \n",
    "    for class_i in range(num_classes):\n",
    "        \n",
    "        # Log transformation applied to prior probability.\n",
    "        log_prior_probability = np.log(prior_probabilities[class_i])\n",
    "        \n",
    "        sum_log_conditional_probabilities = 0\n",
    "        for feature_i in range(len(x)):\n",
    "            \n",
    "            # We add up the log conditional probability of feature (word)\n",
    "            # We multiply log conditional probabilities with the number of times words appeared.\n",
    "            sum_log_conditional_probabilities += np.log(feature_probabilities[feature_i,class_i]) * x[feature_i]\n",
    "        \n",
    "        \n",
    "        log_posterior_probability = log_prior_probability + sum_log_conditional_probabilities\n",
    "        \n",
    "        log_posterior_probabilities[class_i] = log_posterior_probability\n",
    "    \n",
    "    \n",
    "    return log_posterior_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439dcfbf",
   "metadata": {},
   "source": [
    "* We'll try this function below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6f6b0",
   "metadata": {},
   "source": [
    "<div id=\"4.4\">\n",
    "\n",
    "### 4.4 Prediction Function and Testing\n",
    "\n",
    "Now we can define a prediction function to predict bulk samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c62d78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,prior_probabilities,feature_probabilities):\n",
    "    \n",
    "    predictions = np.zeros(len(X))\n",
    "    \n",
    "    for i,x in enumerate(X):\n",
    "        \n",
    "        posterior_probs = calculate_posterior_probabilities(x,prior_probabilities,feature_probabilities)\n",
    "        \n",
    "        predictions[i] = np.argmax(posterior_probs)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397dc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating a train and test set.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c06947",
   "metadata": {},
   "source": [
    "* Now let's calculate prior and posterior probabilities on the train set and calculate posterior probabilities of the test set to see how good our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f26fe0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 98.09%\n"
     ]
    }
   ],
   "source": [
    "prior = calculate_prior_probabilities(y_train)\n",
    "feature_probs = calculate_feature_probabilities(X_train,y_train)\n",
    "\n",
    "y_pred = predict(X_test,prior,feature_probs)\n",
    "\n",
    "correct_preds = np.sum(y_pred == y_test)\n",
    "acc = round(correct_preds / len(y_pred) * 100,2)\n",
    "\n",
    "print(\"Accuracy of the model: {}%\".format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d06244",
   "metadata": {},
   "source": [
    "* This rocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fb879",
   "metadata": {},
   "source": [
    "<div id=\"5\">\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "Naive Bayes is a powerful classification algorithm.\n",
    "1. It's fast. \n",
    "2. It's computation cost is low which makes it possible to use as an online algorithm. \n",
    "    You can always add a few new samples once in a while and update probabilities. It doesn't involve a complex training phase. \n",
    "\n",
    "If you find certain parts, especially parts we've used the numpy API, confusing, it's okay. Numpy API is challenging until you get familiar with it and you'll get used to it as you use it. \n",
    "\n",
    "Thank you for sticking with me until the end, see you in the next tutorials!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
